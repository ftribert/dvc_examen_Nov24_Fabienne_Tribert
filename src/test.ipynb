{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os \n",
    "import pickle\n",
    "import json\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ave_flot_air_flow</th>\n",
       "      <th>ave_flot_level</th>\n",
       "      <th>iron_feed</th>\n",
       "      <th>starch_flow</th>\n",
       "      <th>amina_flow</th>\n",
       "      <th>ore_pulp_flow</th>\n",
       "      <th>ore_pulp_pH</th>\n",
       "      <th>ore_pulp_density</th>\n",
       "      <th>silica_concentrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-04-24 00:00:00</th>\n",
       "      <td>300.263166</td>\n",
       "      <td>383.982443</td>\n",
       "      <td>55.17</td>\n",
       "      <td>1979.589150</td>\n",
       "      <td>599.676489</td>\n",
       "      <td>400.017222</td>\n",
       "      <td>9.774028</td>\n",
       "      <td>1.753206</td>\n",
       "      <td>4.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-24 01:00:00</th>\n",
       "      <td>299.782402</td>\n",
       "      <td>386.049069</td>\n",
       "      <td>55.17</td>\n",
       "      <td>1758.466329</td>\n",
       "      <td>600.043100</td>\n",
       "      <td>400.484528</td>\n",
       "      <td>9.539246</td>\n",
       "      <td>1.754190</td>\n",
       "      <td>3.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-24 02:00:00</th>\n",
       "      <td>299.750052</td>\n",
       "      <td>385.250935</td>\n",
       "      <td>55.17</td>\n",
       "      <td>2379.752428</td>\n",
       "      <td>599.948406</td>\n",
       "      <td>400.325617</td>\n",
       "      <td>9.434227</td>\n",
       "      <td>1.756873</td>\n",
       "      <td>4.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-24 03:00:00</th>\n",
       "      <td>299.997522</td>\n",
       "      <td>389.635519</td>\n",
       "      <td>55.17</td>\n",
       "      <td>2287.130046</td>\n",
       "      <td>599.580383</td>\n",
       "      <td>399.801506</td>\n",
       "      <td>9.725607</td>\n",
       "      <td>1.727125</td>\n",
       "      <td>4.860153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-24 04:00:00</th>\n",
       "      <td>300.005220</td>\n",
       "      <td>387.810807</td>\n",
       "      <td>55.17</td>\n",
       "      <td>2291.789167</td>\n",
       "      <td>599.871217</td>\n",
       "      <td>399.567333</td>\n",
       "      <td>9.845198</td>\n",
       "      <td>1.633063</td>\n",
       "      <td>4.780898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-27 10:00:00</th>\n",
       "      <td>299.958702</td>\n",
       "      <td>466.646630</td>\n",
       "      <td>60.02</td>\n",
       "      <td>3191.856167</td>\n",
       "      <td>319.003256</td>\n",
       "      <td>402.920396</td>\n",
       "      <td>9.294575</td>\n",
       "      <td>1.559673</td>\n",
       "      <td>1.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-27 11:00:00</th>\n",
       "      <td>300.013584</td>\n",
       "      <td>468.726893</td>\n",
       "      <td>60.02</td>\n",
       "      <td>3460.924500</td>\n",
       "      <td>393.149528</td>\n",
       "      <td>397.119469</td>\n",
       "      <td>9.393422</td>\n",
       "      <td>1.696894</td>\n",
       "      <td>1.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-27 12:00:00</th>\n",
       "      <td>300.127863</td>\n",
       "      <td>490.918520</td>\n",
       "      <td>60.02</td>\n",
       "      <td>3382.668556</td>\n",
       "      <td>430.495100</td>\n",
       "      <td>400.424371</td>\n",
       "      <td>9.367542</td>\n",
       "      <td>1.721010</td>\n",
       "      <td>1.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-29 22:00:00</th>\n",
       "      <td>299.670466</td>\n",
       "      <td>445.903411</td>\n",
       "      <td>54.49</td>\n",
       "      <td>2778.946000</td>\n",
       "      <td>563.556606</td>\n",
       "      <td>398.260061</td>\n",
       "      <td>8.844935</td>\n",
       "      <td>1.689140</td>\n",
       "      <td>2.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-08 15:00:00</th>\n",
       "      <td>299.883469</td>\n",
       "      <td>400.304983</td>\n",
       "      <td>59.39</td>\n",
       "      <td>3846.087611</td>\n",
       "      <td>438.641939</td>\n",
       "      <td>401.070041</td>\n",
       "      <td>9.399870</td>\n",
       "      <td>1.605821</td>\n",
       "      <td>1.840000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1817 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ave_flot_air_flow  ave_flot_level  iron_feed  \\\n",
       "2017-04-24 00:00:00         300.263166      383.982443      55.17   \n",
       "2017-04-24 01:00:00         299.782402      386.049069      55.17   \n",
       "2017-04-24 02:00:00         299.750052      385.250935      55.17   \n",
       "2017-04-24 03:00:00         299.997522      389.635519      55.17   \n",
       "2017-04-24 04:00:00         300.005220      387.810807      55.17   \n",
       "...                                ...             ...        ...   \n",
       "2017-08-27 10:00:00         299.958702      466.646630      60.02   \n",
       "2017-08-27 11:00:00         300.013584      468.726893      60.02   \n",
       "2017-08-27 12:00:00         300.127863      490.918520      60.02   \n",
       "2017-08-29 22:00:00         299.670466      445.903411      54.49   \n",
       "2017-09-08 15:00:00         299.883469      400.304983      59.39   \n",
       "\n",
       "                     starch_flow  amina_flow  ore_pulp_flow  ore_pulp_pH  \\\n",
       "2017-04-24 00:00:00  1979.589150  599.676489     400.017222     9.774028   \n",
       "2017-04-24 01:00:00  1758.466329  600.043100     400.484528     9.539246   \n",
       "2017-04-24 02:00:00  2379.752428  599.948406     400.325617     9.434227   \n",
       "2017-04-24 03:00:00  2287.130046  599.580383     399.801506     9.725607   \n",
       "2017-04-24 04:00:00  2291.789167  599.871217     399.567333     9.845198   \n",
       "...                          ...         ...            ...          ...   \n",
       "2017-08-27 10:00:00  3191.856167  319.003256     402.920396     9.294575   \n",
       "2017-08-27 11:00:00  3460.924500  393.149528     397.119469     9.393422   \n",
       "2017-08-27 12:00:00  3382.668556  430.495100     400.424371     9.367542   \n",
       "2017-08-29 22:00:00  2778.946000  563.556606     398.260061     8.844935   \n",
       "2017-09-08 15:00:00  3846.087611  438.641939     401.070041     9.399870   \n",
       "\n",
       "                     ore_pulp_density  silica_concentrate  \n",
       "2017-04-24 00:00:00          1.753206            4.360000  \n",
       "2017-04-24 01:00:00          1.754190            3.290000  \n",
       "2017-04-24 02:00:00          1.756873            4.900000  \n",
       "2017-04-24 03:00:00          1.727125            4.860153  \n",
       "2017-04-24 04:00:00          1.633063            4.780898  \n",
       "...                               ...                 ...  \n",
       "2017-08-27 10:00:00          1.559673            1.690000  \n",
       "2017-08-27 11:00:00          1.696894            1.590000  \n",
       "2017-08-27 12:00:00          1.721010            1.450000  \n",
       "2017-08-29 22:00:00          1.689140            2.970000  \n",
       "2017-09-08 15:00:00          1.605821            1.840000  \n",
       "\n",
       "[1817 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# recup les data raw\n",
    "\n",
    "data_raw = pd.read_csv('../../data/raw_data/raw.csv')\n",
    "\n",
    "display(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La création des fichier train et test s'est bien éfféctuée avec une repartition de 80/20%.\n"
     ]
    }
   ],
   "source": [
    "# tratement de cette donné \n",
    "# SPLIT\n",
    "\n",
    "def Splite(Data_Raw = pd.read_csv('../../data/raw_data/raw.csv')):\n",
    "\n",
    "    data_raw = Data_Raw\n",
    "\n",
    "    # Séparation des variable cible et des Features\n",
    "    df_y = data_raw[['silica_concentrate']]\n",
    "    df_X = data_raw.drop('silica_concentrate', axis=1) \n",
    "    \n",
    "    # Split en 4 DF distincts \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=42)\n",
    "\n",
    "    path = \"../../data/processed_data\"\n",
    "\n",
    "    # Enregistrer les DataFrames en fichiers CSV\n",
    "    X_train.to_csv(os.path.join(path, 'X_train.csv'), index=False)\n",
    "    X_test.to_csv(os.path.join(path, 'X_test.csv'), index=False)\n",
    "    y_train.to_csv(os.path.join(path, 'y_train.csv'), index=False)\n",
    "    y_test.to_csv(os.path.join(path, 'y_test.csv'), index=False)\n",
    "\n",
    "\n",
    "    return print (\"La création des fichier train et test s'est bien éfféctuée avec une repartition de 80/20%.\")\n",
    "\n",
    "Splite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalisation des données effectuée et fichiers sauvegardés.\n"
     ]
    }
   ],
   "source": [
    "def normalize_data(X_train_path='../../data/processed_data/X_train.csv', X_test_path='../../data/processed_data/X_test.csv'):\n",
    "    # Chargement des données\n",
    "    X_train = pd.read_csv(X_train_path)\n",
    "    X_test = pd.read_csv(X_test_path)\n",
    "\n",
    "    # Normalisation des données\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Conversion en DataFrame pour la sauvegarde\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "  \n",
    "    # Définir le chemin où les fichiers CSV seront sauvegardés\n",
    "    path = \"../../data/processed_data\"\n",
    " \n",
    "    # Enregistrer les DataFrames normalisés en fichiers CSV\n",
    "    X_train_scaled_df.to_csv(os.path.join(path, 'X_train_scaled.csv'), index=False)\n",
    "    X_test_scaled_df.to_csv(os.path.join(path, 'X_test_scaled.csv'), index=False)\n",
    "\n",
    "    return print(\"Normalisation des données effectuée et fichiers sauvegardés.\")\n",
    "\n",
    "normalize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Meilleur modèle sauvegardé dans ../../models/best_decision_tree_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# 3 - Grid shearch \n",
    "\n",
    "def grid_search_decision_tree(X_train_path='../../data/processed_data/X_train_scaled.csv', y_train_path='../../data/processed_data/y_train.csv'):\n",
    "    # Chargement des données\n",
    "    X_train = pd.read_csv(X_train_path)\n",
    "    y_train = pd.read_csv(y_train_path)\n",
    "\n",
    "    # Définition de l'arbre de décision\n",
    "    model = DecisionTreeRegressor()\n",
    "\n",
    "    # Définition des paramètres à tester\n",
    "    param_grid = {\n",
    "        'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'max_depth': [None, 5, 10, 15, 20] }\n",
    "\n",
    "    # Configuration de la recherche de grille\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                               scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "    # Exécution de la recherche de grille\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Sauvegarde du meilleur modèle\n",
    "    models_path = \"../../models\"\n",
    "    if not os.path.exists(models_path):\n",
    "        os.makedirs(models_path)\n",
    "        \n",
    "    best_model_filename = os.path.join(models_path, 'best_decision_tree_model.pkl')\n",
    "    with open(best_model_filename, 'wb') as file:\n",
    "        pickle.dump(grid_search.best_estimator_, file)\n",
    "\n",
    "    return print(f\"Meilleur modèle sauvegardé dans {models_path}/best_decision_tree_model.pkl\")\n",
    "\n",
    "grid_search_decision_tree()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle entraîné sauvegardé dans ../../models\\trained_decision_tree_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# entrainement du modele\n",
    "def train_model(X_train_path='../../data/processed_data/X_train_scaled.csv', \n",
    "                y_train_path='../../data/processed_data/y_train.csv',\n",
    "                model_path='../../models/best_decision_tree_model.pkl'):\n",
    "    \n",
    "    # Chargement des données\n",
    "    X_train = pd.read_csv(X_train_path)\n",
    "    y_train = pd.read_csv(y_train_path)\n",
    "\n",
    "    # Chargement du meilleur modèle depuis le fichier pickle\n",
    "    with open(model_path, 'rb') as file:\n",
    "        best_model = pickle.load(file)\n",
    "\n",
    "    # Entraînement du modèle avec les meilleures hyperparamètres\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Sauvegarde du modèle entraîné\n",
    "    trained_model_filename = os.path.join(os.path.dirname(model_path), 'trained_decision_tree_model.pkl')\n",
    "    with open(trained_model_filename, 'wb') as file:\n",
    "        pickle.dump(best_model, file)\n",
    "\n",
    "    return print(f\"Modèle entraîné sauvegardé dans {trained_model_filename}\")\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédictions sauvegardées dans ../../data/predictions.csv\n",
      "Métriques d'évaluation sauvegardées dans ../../metrics/scores.json\n"
     ]
    }
   ],
   "source": [
    "def evaluation(X_test_path='../../data/processed_data/X_test_scaled.csv', \n",
    "               y_test_path='../../data/processed_data/y_test.csv',\n",
    "               model_path='../../models/trained_decision_tree_model.pkl', \n",
    "               predictions_path='../../data/predictions.csv', \n",
    "               metrics_path='../../metrics/scores.json'):\n",
    "    \n",
    "    # Chargement des données\n",
    "    X_test = pd.read_csv(X_test_path)\n",
    "    y_test = pd.read_csv(y_test_path)\n",
    "\n",
    "    # Chargement du modèle entraîné\n",
    "    with open(model_path, 'rb') as file:\n",
    "        trained_model = pickle.load(file)\n",
    "\n",
    "    # Prédictions\n",
    "    predictions = trained_model.predict(X_test)\n",
    "\n",
    "    # Calcul des métriques d'évaluation\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "\n",
    "    # Sauvegarde des prédictions dans un DataFrame\n",
    "    predictions_df = pd.DataFrame({'Actual': y_test.squeeze(), 'Predicted': predictions})\n",
    "    predictions_df.to_csv(predictions_path, index=False)\n",
    "\n",
    "    # Sauvegarde des métriques dans un fichier JSON\n",
    "    metrics = {'MSE': mse, 'R2': r2}\n",
    "    with open(metrics_path, 'w') as json_file:\n",
    "        json.dump(metrics, json_file)\n",
    "\n",
    "    print(f\"Prédictions sauvegardées dans {predictions_path}\")\n",
    "    return    print(f\"Métriques d'évaluation sauvegardées dans {metrics_path}\")\n",
    "\n",
    "\n",
    "evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
